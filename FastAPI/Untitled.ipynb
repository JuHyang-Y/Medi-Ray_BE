{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b8729971-87ee-4991-8e68-dcdd5485e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./MONAI')\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torchvision import models\n",
    "from tensorboardX import SummaryWriter\n",
    "from config import parse_arguments\n",
    "from datasets import DiseaseDataset\n",
    "from utils_folder.utils import AverageMeter, ProgressMeter\n",
    "from utils_folder.eval_metric import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1c5575bd-4521-4ace-8b3e-add4db471eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열로 변환하는 헬퍼 함수\n",
    "# 모델의 출력을 numpy 배열로 변환하기 위한 함수\n",
    "# 입력 텐서를 CPU로 이동시키고, detach하여 연산 그래프에서 분리한 뒤\n",
    "# numpy로 변환한 후, (batch, height, width, channels) 형태로 차원을 재배치합니다.\n",
    "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "# ViT Attention Rollout 클래스\n",
    "# Vision Transformer(ViT) 모델의 attention 맵을 추출하고 시각화하기 위한 클래스\n",
    "class VITAttentionRollout:\n",
    "    def __init__(self, model, attention_layer_name='self_attention', head_fusion=\"mean\", discard_ratio=0.9):\n",
    "        \"\"\"\n",
    "        VITAttentionRollout 초기화 함수\n",
    "        Args:\n",
    "            model: Vision Transformer 모델\n",
    "            attention_layer_name: attention 레이어 이름, forward hook을 등록할 레이어\n",
    "            head_fusion: 여러 attention head를 병합하는 방식 ('mean', 'max', 'min' 중 하나)\n",
    "            discard_ratio: attention을 시각화할 때, 상위 discard_ratio 비율의 주의값을 제거하는 비율\n",
    "        \"\"\"\n",
    "        self.model = model  # ViT 모델 저장\n",
    "        self.head_fusion = head_fusion  # attention head 병합 방식\n",
    "        self.discard_ratio = discard_ratio  # discard ratio 설정\n",
    "        self.attentions = []  # attention 값을 저장할 리스트\n",
    "\n",
    "        # 모델의 지정된 attention 레이어에 forward hook 등록\n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)  # get_attention 함수로 hook 설정\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        \"\"\"\n",
    "        forward hook에서 호출되는 함수로, 모델의 attention 맵을 저장합니다.\n",
    "        Args:\n",
    "            module: 현재 레이어 모듈\n",
    "            input: 레이어로 들어오는 입력 (사용되지 않음)\n",
    "            output: 레이어의 출력, attention 맵을 포함\n",
    "        \"\"\"\n",
    "        # 출력이 tuple 형태이면 첫 번째 요소를 사용하여 attention 맵을 추출하고, CPU로 이동한 뒤 리스트에 추가\n",
    "        attention_map = output[0].cpu() if isinstance(output, tuple) else output.cpu()\n",
    "        self.attentions.append(attention_map)  # attention 맵을 리스트에 저장\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        \"\"\"\n",
    "        모델의 attention 맵을 얻기 위해 객체를 함수처럼 호출합니다.\n",
    "        Args:\n",
    "            input_tensor: 모델에 입력할 이미지 텐서\n",
    "        Returns:\n",
    "            attention_map: attention rollout을 적용한 결과 attention 맵\n",
    "        \"\"\"\n",
    "        self.attentions = []  # 새로운 호출마다 attention 리스트를 초기화\n",
    "        with torch.no_grad():  # backpropagation 비활성화 (메모리 절약)\n",
    "            output = self.model(input_tensor)  # 모델 실행\n",
    "        # 저장된 attention 맵들에 rollout 함수 적용\n",
    "        return rollout(self.attentions, self.discard_ratio, self.head_fusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "570f2f34-6890-402e-a280-96350c745f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(attentions, discard_ratio, head_fusion):\n",
    "    \"\"\"\n",
    "    Rollout 함수를 사용하여 attention 레이어들의 attention 맵을 종합해 시각화 가능한 최종 attention 맵을 생성합니다.\n",
    "    Args:\n",
    "        attentions: attention 레이어들의 attention 맵 리스트\n",
    "        discard_ratio: 낮은 attention 값의 일부를 제거하기 위한 비율\n",
    "        head_fusion: 여러 attention head를 병합하는 방식 ('mean', 'max', 'min' 중 하나)\n",
    "    Returns:\n",
    "        최종적으로 생성된 attention 맵\n",
    "    \"\"\"\n",
    "    # 첫 attention의 시퀀스 길이를 사용하여 초기 result 설정\n",
    "    sequence_length = attentions[0].size(-1)\n",
    "    result = torch.eye(sequence_length).unsqueeze(0).to(attentions[0].device)  # (1, 768, 768)로 초기화\n",
    "    print(\"1. Initial result size:\", result.size())\n",
    "\n",
    "    # 각 attention 레이어를 처리하여 최종 attention 맵을 생성\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            # 여러 attention head 병합\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(dim=1)  # 각 head의 평균\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(dim=1)[0]  # 각 head의 최대값\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(dim=1)[0]  # 각 head의 최소값\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported attention head fusion type\")\n",
    "\n",
    "            print(\"2. attention_heads_fused size:\", attention_heads_fused.size())\n",
    "\n",
    "            # 행렬 곱을 수행하기 위해 a 생성\n",
    "            a = (attention_heads_fused + 1.0) / 2  # attention 값을 0~1로 정규화\n",
    "            a = a / a.sum(dim=-1, keepdim=True)  # 각 행의 합이 1이 되도록 정규화\n",
    "            a = a.unsqueeze(-1)  # (1, 768, 1)로 만듦\n",
    "            print(\"3. a size:\", a.size())\n",
    "\n",
    "            # result와 a의 차원을 맞추고 행렬 곱 수행\n",
    "            print(\"Result before matmul:\", result.size(), \"  a size:\", a.size())\n",
    "            result = torch.matmul(result, a)\n",
    "            print(\"4. result size after matmul:\", result.size())  # 중간 결과 확인용\n",
    "\n",
    "            # result의 마지막 차원이 축소되지 않았는지 확인\n",
    "            if result.size(-1) != sequence_length:\n",
    "                raise RuntimeError(f\"Unexpected result shape: {result.size()}\")\n",
    "\n",
    "    # 최종적으로 클래스 토큰 제외 후 이미지 패치에 대한 attention 맵 생성\n",
    "    mask = result.squeeze(-1)[0, 1:]  # 첫 번째 배치의 첫 번째 토큰 제외\n",
    "    print(\"5. Final mask size:\", mask.size())\n",
    "    width = int(mask.size(-1) ** 0.5)  # 이미지 크기 추정\n",
    "    mask = mask.reshape(width, width).cpu().numpy()  # 2D 형태로 변환 후 numpy로 변환\n",
    "    mask = mask / np.max(mask)  # 0~1로 정규화\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7248d74a-3282-4cb2-a37a-22d5580d093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask_on_image(img, mask):\n",
    "    \"\"\"\n",
    "    입력 이미지 위에 attention 맵(mask)을 겹쳐서 시각화하는 함수입니다.\n",
    "    Args:\n",
    "        img: 원본 이미지 (numpy 배열)\n",
    "        mask: attention 맵 (numpy 배열, 0~1 범위로 정규화된 형태)\n",
    "    Returns:\n",
    "        원본 이미지 위에 attention 맵을 덧씌운 결과 이미지 (numpy 배열, uint8 형식)\n",
    "    \"\"\"\n",
    "    # 원본 이미지를 0~1 사이로 정규화\n",
    "    img = np.float32(img) / 255\n",
    "\n",
    "    # attention 맵을 히트맵으로 변환\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)  # mask를 0~255 사이로 스케일링 후 COLORMAP_JET 적용\n",
    "    heatmap = np.float32(heatmap) / 255  # 0~1 사이로 정규화하여 원본 이미지와 결합 준비\n",
    "\n",
    "    # 원본 이미지에 히트맵을 겹침\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)  # 전체 픽셀 값의 최대값을 1로 정규화\n",
    "\n",
    "    # 최종 이미지를 uint8 형식으로 변환하여 반환 (0~255 사이 값)\n",
    "    return np.uint8(255 * cam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2824db76-ec0b-4ae4-b43a-7ef51a543af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cam(args, loader, model, device, num_classes, class_list, log_dir):\n",
    "    \"\"\"\n",
    "    ViT 모델의 attention 맵을 생성하고 원본 이미지와 결합하여 시각화하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        args: 설정 파라미터를 포함한 객체\n",
    "        loader: 데이터 로더 객체\n",
    "        model: Vision Transformer(ViT) 모델\n",
    "        device: 모델과 데이터를 실행할 장치(CPU 또는 GPU)\n",
    "        num_classes: 분류할 클래스의 수\n",
    "        class_list: 클래스 이름 목록\n",
    "        log_dir: 시각화 결과를 저장할 디렉토리 경로\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    save_dir = log_dir + '/vit_attention'  # 결과 저장 경로 설정\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 저장 경로가 없으면 생성\n",
    "\n",
    "    # ViT Attention Rollout 객체 생성\n",
    "    vit_attention = VITAttentionRollout(model)\n",
    "\n",
    "    # 데이터 로더를 통해 이미지와 라벨을 가져옴\n",
    "    for iter_, (imgs, labels) in enumerate(iter(loader)):\n",
    "        print(f\"Iteration {iter_}: imgs shape = {imgs.shape}, labels shape = {labels.shape}\")  # 이미지와 라벨 크기 출력\n",
    "\n",
    "        imgs = imgs.to(device)  # 이미지를 장치로 이동\n",
    "        labels = labels.to(device, dtype=torch.long)  # 라벨을 장치로 이동\n",
    "\n",
    "        # Attention 히트맵 생성\n",
    "        attention_mask = vit_attention(imgs)  # ViT 모델로부터 attention 맵 생성\n",
    "        print(f\"Attention mask shape: {attention_mask.shape}\")  # Attention 마스크 크기 확인\n",
    "\n",
    "        # attention 맵을 원본 이미지 크기로 리사이즈\n",
    "        attention_mask = cv2.resize(attention_mask, (imgs.shape[-1], imgs.shape[-2]))\n",
    "\n",
    "        # 이미지를 numpy 배열로 변환\n",
    "        gt_imgs = fn_tonumpy(imgs)\n",
    "        print(f\"gt_imgs shape: {gt_imgs.shape}\")  # 배열 크기 확인\n",
    "\n",
    "        cam_imgs = gt_imgs[0].squeeze()  # 첫 번째 배치의 이미지 선택 및 차원 축소\n",
    "\n",
    "        # attention 맵을 원본 이미지에 겹침\n",
    "        attention_result = show_mask_on_image(cam_imgs, attention_mask)\n",
    "\n",
    "        # 결합된 이미지 생성: 왼쪽에 원본 이미지, 오른쪽에 attention 맵이 적용된 이미지\n",
    "        full_image = np.zeros((224, 448, 3))  # 빈 이미지 생성\n",
    "        full_image[:224, :224, :] = cam_imgs * 255  # 왼쪽에 원본 이미지 추가\n",
    "        full_image[:224, 224:448, :] = attention_result  # 오른쪽에 attention 이미지 추가\n",
    "\n",
    "        # 결과 이미지를 파일로 저장\n",
    "        img_path = os.path.join(save_dir, f'{iter_}_attention.jpg')\n",
    "        cv2.imwrite(img_path, full_image)\n",
    "\n",
    "        # 이미지 시각화\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(cam_imgs, cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(attention_result[..., ::-1])  # 컬러 순서 변경하여 출력\n",
    "        plt.title('Attention Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 실제 라벨을 문자열로 변환하여 출력\n",
    "        actual_labels = labels[0].cpu().numpy()  # 첫 번째 이미지의 라벨을 가져옴\n",
    "        actual_labels_str = ', '.join([class_list[i] for i in range(num_classes) if actual_labels[i] == 1])\n",
    "        plt.suptitle(f'Actual Labels: {actual_labels_str}')\n",
    "        plt.show()\n",
    "\n",
    "        # 실제 라벨 정보 출력\n",
    "        print(\"Actual Labels: \", actual_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "de6300bb-4a52-42a4-bcb1-c8dd37e903dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Clearing GPU memory...\n",
      "[INFO] GPU memory cleared.\n",
      "[INFO] Using device: cuda:1\n",
      "[*] build network... backbone: vit\n",
      "Load model completed\n",
      "[*] start a test\n",
      "Iteration 0: imgs shape = torch.Size([1, 3, 224, 224]), labels shape = torch.Size([1, 14])\n",
      "1. Initial result size: torch.Size([1, 768, 768])\n",
      "2. attention_heads_fused size: torch.Size([1, 768])\n",
      "3. a size: torch.Size([1, 768, 1])\n",
      "Result before matmul: torch.Size([1, 768, 768])   a size: torch.Size([1, 768, 1])\n",
      "4. result size after matmul: torch.Size([1, 768, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected result shape: torch.Size([1, 768, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m     evaluate_cam(args, test_loader, model, device, args\u001b[38;5;241m.\u001b[39mnum_class, class_list, args\u001b[38;5;241m.\u001b[39mlog_dir)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[94], line 76\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# 테스트 시작\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[*] start a test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m \u001b[43mevaluate_cam\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[93], line 29\u001b[0m, in \u001b[0;36mevaluate_cam\u001b[0;34m(args, loader, model, device, num_classes, class_list, log_dir)\u001b[0m\n\u001b[1;32m     26\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# 라벨을 장치로 이동\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Attention 히트맵 생성\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mvit_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ViT 모델로부터 attention 맵 생성\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention mask shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Attention 마스크 크기 확인\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# attention 맵을 원본 이미지 크기로 리사이즈\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[90], line 53\u001b[0m, in \u001b[0;36mVITAttentionRollout.__call__\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     51\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(input_tensor)  \u001b[38;5;66;03m# 모델 실행\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 저장된 attention 맵들에 rollout 함수 적용\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscard_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead_fusion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 44\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(attentions, discard_ratio, head_fusion)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;66;03m# result의 마지막 차원이 축소되지 않았는지 확인\u001b[39;00m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m sequence_length:\n\u001b[0;32m---> 44\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# 최종적으로 클래스 토큰 제외 후 이미지 패치에 대한 attention 맵 생성\u001b[39;00m\n\u001b[1;32m     47\u001b[0m mask \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# 첫 번째 배치의 첫 번째 토큰 제외\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected result shape: torch.Size([1, 768, 1])"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pathlib\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json  # json 모듈 임포트\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def main():\n",
    "    ##### GPU 메모리 정리\n",
    "    # 이전에 사용된 GPU 메모리를 정리\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Clearing GPU memory...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        print(\"[INFO] GPU memory cleared.\")\n",
    "\n",
    "    ##### GPU 1번 지정\n",
    "    # 특정 GPU를 사용하도록 설정 (GPU 1번)\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    ##### Initial Settings\n",
    "\n",
    "    filename = '/211113_vit_b16_uni_224_64_1e-3'\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.num_class = 14\n",
    "            self.backbone = 'vit'\n",
    "            self.log_dir = './runs' + filename\n",
    "            self.img_size = 224\n",
    "            self.bits = 8\n",
    "            self.seed = 50\n",
    "            self.w = 4\n",
    "            self.resume = True\n",
    "            self.pretrained = './runs' + filename + '/best.pth.tar'\n",
    "\n",
    "    args = Args()\n",
    "\n",
    "    # 클래스 리스트 정의\n",
    "    class_list = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', \n",
    "                   'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "                   'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other',\n",
    "                   'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # 모델 선택 및 로드\n",
    "    print('[*] build network... backbone: {}'.format(args.backbone))\n",
    "    if args.backbone == 'vit':\n",
    "        vit = models.vit_b_16(pretrained=True)\n",
    "        head_in_features = vit.heads.head.in_features\n",
    "        vit.heads.head = nn.Linear(head_in_features, 14)\n",
    "        model = vit\n",
    "    else:\n",
    "        raise ValueError('Have to set the backbone network in [resnet, vgg, densenet]')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.pretrained, map_location=device)\n",
    "        pretrained_dict = checkpoint['state_dict']\n",
    "        pretrained_dict = {key.replace(\"module.\", \"\"): value for key, value in pretrained_dict.items()}\n",
    "        model.load_state_dict(pretrained_dict)\n",
    "        print(\"Load model completed\")\n",
    "    else:\n",
    "        raise ValueError('Have to input a pretrained network path')\n",
    "\n",
    "    # 데이터셋 및 데이터로더 설정\n",
    "    test_datasets = DiseaseDataset('./json' + filename + '.json', 'test', args.img_size, args.bits, args)\n",
    "    test_loader = torch.utils.data.DataLoader(test_datasets, batch_size=1,\n",
    "                                              num_workers=args.w, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # 테스트 시작\n",
    "    print('[*] start a test')\n",
    "    evaluate_cam(args, test_loader, model, device, args.num_class, class_list, args.log_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793ed8e-b027-4201-8922-f27ba039cdc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb00f4-dfc7-4c78-bc87-64df8156b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
